---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi~ I am **Wangchunshu Zhou**, a second-year master student from the [Department of Computer Science and Technology](http://scse.buaa.edu.cn/), Beihang University. Before that, I received my Bachelor's degree from the [Sino-French Engineering School](http://ecpkn.buaa.edu.cn/), Beihang University.

I am a member of the National Laboratory of Software Development Environment (NLSDE) and advised by Professor [Ke Xu](http://sites.nlsde.buaa.edu.cn/~kexu/). I am currently a research intern at [NLC Group @ Microsoft Research Asia](https://www.microsoft.com/en-us/research/group/natural-language-computing/) and advised Doctor Tao Ge. Please check my [CV](/files/CV-2020.4.5.pdf) for further information.

**I will apply for Ph.D. in NLP, Deep Learning, and related areas starting from Fall 2021. Feel free to drop me a line if interested!**

Research
======
My research interests are mainly applying Deep Learning for Natural Language Processing. I'm currently interested in **Natural language Generation**, including creative text generation applications, better NLG models, and better evaluation method for NLG systems. I am also interested in making large-scale pretrained language models more accessible to both academic and industry applications. To achieve this, I am actively working on **Efficient Pretrained Language Modeling** and **Transfer Learning for NLP**. This includes efficiency in terms of both the amount of **data** and **time** required for pretraining and fine-tuning pretrained langauge models, as well as the number of parameters and computational cost, which affects the memory requirement and latency for application of pretrained language models. 

I am also interested in or working in the following subjects:</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Commonsense reasoning and knowledge-based reasoning.**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Interpretability, explainability for NLP models, bias and fairness in NLP models, and their interactions.**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Low resource NLP and Multilingual Pretrained Language Models.**

News
======
\[2020.9]. Got four first-authored paper accepted to EMNLP 2020 (1 main conference and 3 findings(with average score of 3.67)) and one second author paper accepted to Finding of EMNLP 2020.

\[2019.12]. Got one first-authored paper "**Self-Adversarial Learning with Comparative Discrimination for Text Generation**" accepted by **ICLR 2020**. See you in Addis Ababa.  

\[2019.11]. Got one first-authored paper "**Learning to Compare for Better Training and Evaluation of Open Domain Text Generation Models**" accepted as **Oral** presentation by **AAAI 2020**. See you in New York.   

\[2019.5\] Got one first-authored paper "**BERT-based Lexical Substitution**" accepted by **ACL 2019**. See you in Firenze.  

\[2018.12\] Start my research internship at at [NLC Group @ Microsoft Research Asia](https://www.microsoft.com/en-us/research/group/natural-language-computing/), advised by Dr. Tao Ge.  

\[2018.8\] Start my Master study at NLSDE Lab in Beihang University, advised by Prof. Ke Xu  

Personal information
------
I am a big fan of Harry Potter, Real Madrid, and Cristinao Ronaldo. I enjoy reading books (especially sci-fictions) and playing games (including FIFA, League of Legends, etc.) in my free time.
